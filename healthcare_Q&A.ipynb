{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+tXsKISH4UQpl8fGEkp3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalyanianikkath/NM-Genrative-AI/blob/main/healthcare_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qg6D3qrZooUF",
        "outputId": "1dd0d2dc-55db-4496-b86b-78894f6c6f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.51)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.28)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'GoogleGenerativeAIEmbeddings' from 'langchain.embeddings' (/usr/local/lib/python3.11/dist-packages/langchain/embeddings/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c064b74c7d06>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleGenerativeAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'GoogleGenerativeAIEmbeddings' from 'langchain.embeddings' (/usr/local/lib/python3.11/dist-packages/langchain/embeddings/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ‚úÖ Step 1: Install dependencies\n",
        "!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\n",
        "!pip install -U langchain-community\n",
        "\n",
        "# ‚úÖ Step 2: Import necessary libraries\n",
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from Bio import Entrez\n",
        "\n",
        "# ‚úÖ Step 3: API keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCW5HzSyVUTqh7VbS6nFUiJsoo2fKSjbwM\"  # Replace this with your Gemini API key\n",
        "Entrez.email = \"kalyaniaanikkath@gmail.com\"  # Replace this with your email for PubMed API\n",
        "\n",
        "# ‚úÖ Step 4: Fetch articles from PubMed\n",
        "def fetch_pubmed_articles(query, max_results=5):\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    ids = record[\"IdList\"]\n",
        "    abstracts = []\n",
        "    for pmid in ids:\n",
        "        fetch = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\", retmode=\"text\")\n",
        "        abstract_text = fetch.read()\n",
        "        abstracts.append(abstract_text)\n",
        "    return abstracts\n",
        "\n",
        "# ‚úÖ Step 5: Build vector store from articles\n",
        "def build_vectorstore_from_articles(articles):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    texts = text_splitter.create_documents(articles)\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# ‚úÖ Step 6: Create QA chain\n",
        "def create_qa_chain(vectorstore):\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.2)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "    return qa_chain\n",
        "\n",
        "# ‚úÖ Step 7: Ask a health question\n",
        "def ask_health_question(query, qa_chain):\n",
        "    result = qa_chain(query)\n",
        "    print(\"ü©∫ Answer:\\n\")\n",
        "    print(result[\"result\"])\n",
        "    print(\"\\nüìö Sources:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"]):\n",
        "        print(f\"\\nSource {i+1}:\\n{doc.page_content[:500]}...\")  # show a snippet\n",
        "\n",
        "# ‚úÖ Step 8: Run the chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    user_query = \"What are the latest treatments for type 2 diabetes?\"  # ‚Üê You can change the question\n",
        "    print(\"üîç Searching PubMed...\")\n",
        "    articles = fetch_pubmed_articles(user_query, max_results=5)\n",
        "\n",
        "    print(\"üìö Building vector store...\")\n",
        "    vectorstore = build_vectorstore_from_articles(articles)\n",
        "\n",
        "    print(\"ü§ñ Initializing Q&A chain with Gemini...\")\n",
        "    qa_chain = create_qa_chain(vectorstore)\n",
        "\n",
        "    print(\"\\nüí¨ Asking your question...\\n\")\n",
        "    ask_health_question(user_query, qa_chain)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install dependencies\n",
        "!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\n",
        "\n",
        "# ‚úÖ Step 2: Import necessary libraries\n",
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from Bio import Entrez\n",
        "\n",
        "# ‚úÖ Step 3: API keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCW5HzSyVUTqh7VbS6nFUiJsoo2fKSjbwM\"  # Replace this with your Gemini API key\n",
        "Entrez.email = \"kalyaniaanikkath@gmail.com\"  # Replace this with your email for PubMed API\n",
        "\n",
        "# ‚úÖ Step 4: Fetch articles from PubMed\n",
        "def fetch_pubmed_articles(query, max_results=5):\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    ids = record[\"IdList\"]\n",
        "    abstracts = []\n",
        "    for pmid in ids:\n",
        "        fetch = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\", retmode=\"text\")\n",
        "        abstract_text = fetch.read()\n",
        "        abstracts.append(abstract_text)\n",
        "    return abstracts\n",
        "\n",
        "# ‚úÖ Step 5: Build vector store from articles\n",
        "def build_vectorstore_from_articles(articles):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    texts = text_splitter.create_documents(articles)\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# ‚úÖ Step 6: Create QA chain\n",
        "def create_qa_chain(vectorstore):\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "    return qa_chain\n",
        "\n",
        "# ‚úÖ Step 7: Ask a health question\n",
        "def ask_health_question(query, qa_chain):\n",
        "    result = qa_chain(query)\n",
        "    print(\"ü©∫ Answer:\\n\")\n",
        "    print(result[\"result\"])\n",
        "    print(\"\\nüìö Sources:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"]):\n",
        "        print(f\"\\nSource {i+1}:\\n{doc.page_content[:500]}...\")  # show a snippet\n",
        "\n",
        "# ‚úÖ Step 8: Run the chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    user_query = \"What are the latest treatments for type 2 diabetes?\"  # ‚Üê You can change the question\n",
        "    print(\"üîç Searching PubMed...\")\n",
        "    articles = fetch_pubmed_articles(user_query, max_results=5)\n",
        "\n",
        "    print(\"üìö Building vector store...\")\n",
        "    vectorstore = build_vectorstore_from_articles(articles)\n",
        "\n",
        "    print(\"ü§ñ Initializing Q&A chain with Gemini...\")\n",
        "    qa_chain = create_qa_chain(vectorstore)\n",
        "\n",
        "    print(\"\\nüí¨ Asking your question...\\n\")\n",
        "    ask_health_question(user_query, qa_chain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlQHY82bvWX4",
        "outputId": "f5d5e570-9827-484b-cd71-f81bf548b445"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching PubMed...\n",
            "üìö Building vector store...\n",
            "ü§ñ Initializing Q&A chain with Gemini...\n",
            "\n",
            "üí¨ Asking your question...\n",
            "\n",
            "ü©∫ Answer:\n",
            "\n",
            "Based on the provided texts, flash glucose monitoring and adherence to the 2023 ADA Standards of Care, including a diabetes care bundle, are discussed as treatments and management strategies for type 2 diabetes.  Specific medications within these guidelines are not detailed in this text.\n",
            "\n",
            "üìö Sources:\n",
            "\n",
            "Source 1:\n",
            "FSL for 24‚Äâmonths' follow-up. HCRU was measured for 12‚Äâmonths before the index \n",
            "date and the last 12‚Äâmonths of the 24-month follow-up period. HbA1c data were \n",
            "taken from the latest tests in each period.\n",
            "RESULTS: Mean HbA1c was statistically significantly reduced after FSL \n",
            "acquisition among people aged ‚â§65 or‚Äâ>65‚Äâyears in all four treatment groups \n",
            "(range, 0.3-0.8% reduction). After FSL acquisition, ED visits and \n",
            "hospitalization were statistically significantly reduced in the oral therapy \n",
            "only...\n",
            "\n",
            "Source 2:\n",
            "Promoting provider adherence to American Diabetes Association guidelines with a \n",
            "diabetes care bundle: A DNP quality improvement project.\n",
            "\n",
            "Noor B, Benton K, Vazquez C.\n",
            "\n",
            "Type 2 diabetes mellitus (T2DM) is a common and expensive health condition. \n",
            "Patients are at increased risk for cardiorenal complications when metabolic \n",
            "targets for hemoglobin A1C, BP, and low-density lipoprotein cholesterol are \n",
            "unmet. Many providers do not fully adhere to the latest diabetes guidelines. \n",
            "This quality improveme...\n",
            "\n",
            "Source 3:\n",
            "Author information:\n",
            "(1)Professional Degree Program, School of Public Health, Graduate School of \n",
            "Medicine, The University of Tokyo, Tokyo, Japan.\n",
            "(2)Department of Biomedical Informatics, Graduate School of Medicine, The \n",
            "University of Tokyo, Tokyo, Japan.\n",
            "(3)Department of Diabetes and Metabolic Diseases, Graduate School of Medicine, \n",
            "The University of Tokyo, Tokyo, Japan.\n",
            "(4)Division of Diabetes, Mitsui Memorial Hospital, Tokyo, Japan.\n",
            "(5)Department of Metabolism and Endocrinology, Akita Univers...\n",
            "\n",
            "Source 4:\n",
            "Copyright ¬© 2025 Wolters Kluwer Health, Inc. All rights reserved.\n",
            "\n",
            "DOI: 10.1097/01.NPR.0000000000000300\n",
            "PMID: 40128208 [Indexed for MEDLINE]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install dependencies\n",
        "!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\n",
        "\n",
        "# ‚úÖ Step 2: Import necessary libraries\n",
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from Bio import Entrez\n",
        "\n",
        "# ‚úÖ Step 3: API keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_gemini_api_key_here\"  # Replace with your Gemini API key\n",
        "Entrez.email = \"your_email@example.com\"  # Replace with your email for PubMed\n",
        "\n",
        "# ‚úÖ Step 4: Fetch articles from PubMed\n",
        "def fetch_pubmed_articles(query, max_results=5):\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    ids = record[\"IdList\"]\n",
        "    abstracts = []\n",
        "    for pmid in ids:\n",
        "        fetch = Entrez.efetch(db\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "o4C5FPiVwIYm",
        "outputId": "c8f5edfd-7370-4480-d125-9794f91578a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-10-94ae1912d123>, line 23)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-94ae1912d123>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    fetch = Entrez.efetch(db\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install dependencies\n",
        "!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\n",
        "\n",
        "# ‚úÖ Step 2: Import necessary libraries\n",
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from Bio import Entrez\n",
        "\n",
        "# ‚úÖ Step 3: API keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCW5HzSyVUTqh7VbS6nFUiJsoo2fKSjbwM\"  # Replace with your Gemini API key\n",
        "Entrez.email = \"kalyaniaanikkath@gmail.com\"  # Replace with your email for PubMed\n",
        "\n",
        "# ‚úÖ Step 4: Fetch articles from PubMed\n",
        "def fetch_pubmed_articles(query, max_results=5):\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    ids = record[\"IdList\"]\n",
        "    abstracts = []\n",
        "    for pmid in ids:\n",
        "        fetch = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\", retmode=\"text\")\n",
        "        abstract_text = fetch.read()\n",
        "        abstracts.append(abstract_text)\n",
        "    return abstracts\n",
        "\n",
        "# ‚úÖ Step 5: Build vector store\n",
        "def build_vectorstore_from_articles(articles):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    texts = text_splitter.create_documents(articles)\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# ‚úÖ Step 6: Create Gemini-based QA system\n",
        "def create_qa_chain(vectorstore):\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "    return qa_chain\n",
        "\n",
        "# ‚úÖ Step 7: Ask your question\n",
        "def ask_health_question(query, qa_chain):\n",
        "    result = qa_chain(query)\n",
        "    print(\"\\nü©∫ Answer:\\n\")\n",
        "    print(result[\"result\"])\n",
        "    print(\"\\nüìö Sources:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"]):\n",
        "        print(f\"\\nSource {i+1}:\\n{doc.page_content[:500]}...\")\n",
        "\n",
        "# ‚úÖ Step 8: Run everything interactively\n",
        "if __name__ == \"__main__\":\n",
        "    user_query = input(\"üí¨ Enter your medical/healthcare question: \")\n",
        "\n",
        "    print(\"\\nüîç Searching PubMed for related research...\")\n",
        "    articles = fetch_pubmed_articles(user_query, max_results=5)\n",
        "\n",
        "    if not articles:\n",
        "        print(\"‚ùå No articles found on this topic. Try a different question.\")\n",
        "    else:\n",
        "        print(\"üìö Building knowledge base from PubMed articles...\")\n",
        "        vectorstore = build_vectorstore_from_articles(articles)\n",
        "\n",
        "        print(\"ü§ñ Connecting to Gemini for answer generation...\")\n",
        "        qa_chain = create_qa_chain(vectorstore)\n",
        "\n",
        "        ask_health_question(user_query, qa_chain)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3TXjysvwbZo",
        "outputId": "9552357b-8dcd-47d4-b961-1334711b859b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí¨ Enter your medical/healthcare question: Coronary artery disease mechanisms\n",
            "\n",
            "üîç Searching PubMed for related research...\n",
            "üìö Building knowledge base from PubMed articles...\n",
            "ü§ñ Connecting to Gemini for answer generation...\n",
            "\n",
            "ü©∫ Answer:\n",
            "\n",
            "Excessive production of reactive oxygen species (ROS) contributes to endothelial dysfunction, inflammation, and vascular remodeling.  These pathological effects drive atherosclerosis, plaque formation, and thrombosis.\n",
            "\n",
            "üìö Sources:\n",
            "\n",
            "Source 1:\n",
            "Oxidative stress plays a pivotal role in the pathogenesis of atherosclerosis and \n",
            "coronary artery disease (CAD), with both beneficial and detrimental effects on \n",
            "cardiovascular health. On one hand, the excessive production of reactive oxygen \n",
            "species (ROS) contributes to endothelial dysfunction, inflammation, and vascular \n",
            "remodeling, which are central to the development and progression of CAD. These \n",
            "pathological effects drive key processes such as atherosclerosis, plaque \n",
            "formation, and thromb...\n",
            "\n",
            "Source 2:\n",
            "targeted therapies. The future directions for research aimed at harnessing the \n",
            "diagnostic and therapeutic potential of oxidative stress biomarkers are also \n",
            "discussed. Understanding the balance between the detrimental and beneficial \n",
            "effects of oxidative stress could lead to innovative approaches in the \n",
            "prevention and management of CAD....\n",
            "\n",
            "Source 3:\n",
            "response to prevent infections. Additionally, oxidative stress can stimulate \n",
            "cellular adaptation to stress, promote cell survival, and encourage \n",
            "angiogenesis, which helps form new blood vessels to improve blood flow. \n",
            "Oxidative stress also holds promise as a source of biomarkers that could aid in \n",
            "the diagnosis, prognosis, and monitoring of CAD. Specific oxidative markers, \n",
            "such as malondialdehyde (MDA), isoprostanes (isoP), ischemia-modified albumin, \n",
            "and antioxidant enzyme activity, have bee...\n",
            "\n",
            "Source 4:\n",
            "1. Antioxidants (Basel). 2025 Feb 26;14(3):275. doi: 10.3390/antiox14030275.\n",
            "\n",
            "The Dual Role of Oxidative Stress in Atherosclerosis and Coronary Artery \n",
            "Disease: Pathological Mechanisms and Diagnostic Potential.\n",
            "\n",
            "Myszko M(1), Bychowski J(1), Skrzydlewska E(2), ≈Åuczaj W(2).\n",
            "\n",
            "Author information:\n",
            "(1)Department of Cardiology, Bialystok Regional Hospital, M. Sk≈Çodowskiej-Curie \n",
            "25, 15-950 Bialystok, Poland.\n",
            "(2)Department of Analytical Chemistry, Medical University of Bialystok, \n",
            "Mickiewicza 2d, 15-222...\n"
          ]
        }
      ]
    }
  ]
}